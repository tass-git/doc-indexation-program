{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1HhmV_rjW7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ceb3dce-1908-4018-9e41-3f8673f89377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc a été crér avec succées\n",
            "please enter your request: homme soleil\n",
            "\n",
            " indexed request:  ['homme', 'soleil']\n",
            "\n",
            " request weight:  {'homme': 0.5, 'soleil': 0.5}\n",
            "\n",
            " similarity_score:  {'doc1.txt': 0.04173919355648411, 'doc5.txt': 0.03990434422338111, 'doc7.txt': 0.06835859270246632, 'doc9.txt': 0.042796049251091296}\n",
            "\n",
            " enter number of relevant ocs to show: 3\n",
            "[('doc7.txt', 0.06835859270246632), ('doc9.txt', 0.042796049251091296), ('doc1.txt', 0.04173919355648411)]\n"
          ]
        }
      ],
      "source": [
        "#importing diffrent libraries that are necessary\n",
        "#import the library nltk responsible for different natural language tools\n",
        "import nltk\n",
        "#import regular rexpression library\n",
        "import re\n",
        "#import math library to use sqrt later\n",
        "from math import *\n",
        "#import the rooting library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#import stopwords from nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "#create instance of the class porterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#downloads stopwords initially in english\n",
        "nltk.download('stopwords')\n",
        "#change stopwords to french\n",
        "stop=stopwords.words('french')\n",
        "\n",
        "#list of source docs\n",
        "source=[\"doc1.txt\",\"doc2.txt\",\"doc3.txt\",\"doc4.txt\",\"doc5.txt\",\"doc6.txt\",\"doc7.txt\",\"doc8.txt\",\"doc9.txt\",\"doc10.txt\"]\n",
        "\n",
        "#indexation function\n",
        "def liste_Stem(contenu,stop):\n",
        "  #create a list of 'mots' containing the content 'contenu' in lowercase\n",
        "  mots=re.split(r\"\\W\", contenu.lower())\n",
        "  #create list v  containing the words of mots  that differ from stopwords\n",
        "  v=[t for t in mots if t not in stop]\n",
        "  #create list filtered_words which contains the words of v that dosen't match date format\n",
        "  filtered_words = [t for t in v if not re.match(r\"\\d+\", t)]\n",
        "  #create empty list 'mots_final'\n",
        "  mots_final=[]\n",
        "  #loop the filtered_words\n",
        "  for c in filtered_words:\n",
        "      #root 'raciniser\" every word of  filtered_words\n",
        "      x=stemmer.stem(c)\n",
        "      #create list of rooted words\n",
        "      mots_final.append(x)\n",
        "  return mots\n",
        "\n",
        "#test stemming\n",
        "#mots=liste_Stem(contenu,stop)\n",
        "#print(mots)\n",
        "\n",
        "#fonction calcul local de tf\n",
        "def calcul_TF(contenu):\n",
        "  #create empty dict\n",
        "   d={}\n",
        "   # the total counts of words is the length of contenu\n",
        "   total = len(contenu)\n",
        "   #create empty dict for term frequency\n",
        "   tf={}\n",
        "   #iterate on each word in the content\n",
        "   for word in contenu:\n",
        "    # if the word is not in the dict\n",
        "    if word not in d:\n",
        "      #word hzs value 0\n",
        "      d[word]=0\n",
        "    #incriment word value\n",
        "    d[word] += 1\n",
        "    #term freq is frequency of term in doc / total counts of all words in the doc\n",
        "    tf[word] = d[word] / total\n",
        "   return tf\n",
        "\n",
        "#tf=calcul_TF(mots)\n",
        "#print(\"tf est\",tf)\n",
        "\n",
        "\n",
        "#fonction calcul global tf\n",
        "def TF_global(source):\n",
        "    #create empty dict\n",
        "    d = {}\n",
        "    # the total counts of words is intially 0\n",
        "    total_words = 0\n",
        "    #iterate on all documents in the provided source\n",
        "    for fich in source:\n",
        "        #open the content of each doc to read it\n",
        "        contenu = open(fich, \"r\").read()\n",
        "        #stem the content\n",
        "        mots = liste_Stem(contenu,stop)\n",
        "        #increment count of total word by the length of each doc content\n",
        "        total_words += len(mots)\n",
        "        # call for term freq function\n",
        "        tf = calcul_TF(mots)\n",
        "        #ierate on each word on term freq\n",
        "        for word in tf:\n",
        "             # if the word is not in the dict\n",
        "            if word not in d:\n",
        "                #word count is set to 0\n",
        "                d[word] = 0\n",
        "            #increment term freq of word\n",
        "            d[word] += tf[word]\n",
        "    # Normalisation TF by division tf ib total number of words\n",
        "    for word in d:\n",
        "        d[word] /= total_words\n",
        "\n",
        "    return d\n",
        "\n",
        "#d2 = TF_global(source)\n",
        "#print(\"Global TF:\", d2)\n",
        "\n",
        "#function to calculate weight of request\n",
        "def calculate_request_weight(indexed_req):\n",
        "    #create empty dict\n",
        "    request_weight = {}\n",
        "    #total words is length of request\n",
        "    total_words = len(indexed_req)\n",
        "    # Calculate term frequency (TF) for each token in the query\n",
        "    for token in indexed_req:\n",
        "        if token not in request_weight:\n",
        "            #TF token is set to 0\n",
        "            request_weight[token] = 0\n",
        "        request_weight[token] += 1 / total_words  # TF for the request is uniform for each token\n",
        "\n",
        "    return request_weight\n",
        "\n",
        "#function to calculate weight of doc\n",
        "def calcul_weight_doc(source):\n",
        "    #create empty dict\n",
        "    d = {}\n",
        "    #total docs is length of request\n",
        "    total_docs = len(source)\n",
        "    #iterate on fich in the privided source\n",
        "    for fich in source:\n",
        "        #open the content of each doc to read it\n",
        "        contenu = open(fich, \"r\").read()\n",
        "        #root the content of each doc while getting rid of stopwords\n",
        "        stems = liste_Stem(contenu,stop)\n",
        "        # call for term freq function\n",
        "        tf = calcul_TF(stems)\n",
        "        #iterate on stem,weight in dict of tf values\n",
        "        for stem, weight in tf.items():\n",
        "            if stem not in d:\n",
        "                #create empty dict for each unfound stem\n",
        "                d[stem] = {}\n",
        "            #affect weight for each doc if each stem\n",
        "            d[stem][fich] = weight\n",
        "    return d\n",
        "\n",
        "#d=calcul_weight_doc(source)\n",
        "#print (d)\n",
        "\n",
        "\n",
        "#function to create inversed doc\n",
        "def create_index_inv(source):\n",
        "  #calculate weight of provided source\n",
        "  w=calcul_weight_doc(source)\n",
        "  #open new file to write result in\n",
        "  with open(r\"C:\\Users\\TASNIME\\Documents\\indx mult\\fich-inv.txt\", \"w\") as file:\n",
        "    file.write(\"stem----doc(i)-----W(i).......\\n\")\n",
        "    #iterate on each doc in source\n",
        "    for fich in source:\n",
        "      #open the content of each doc to read it\n",
        "      contenu=open(fich, \"r\").read()\n",
        "      #root the content of each doc while getting rid of stopwords\n",
        "      stems = liste_Stem(contenu,stop)\n",
        "      #iterate on each stem\n",
        "      for s in stems:\n",
        "        #write on the file the stem,doc and weight of each stem\n",
        "        file.write(s+\"----\"+fich+\"----\"+str(w[s])+\"\\n\")\n",
        "  print(\"doc a été crér avec succées\")\n",
        "\n",
        "\n",
        "#function of user request\n",
        "def saisie():\n",
        "  req=input(\"please enter your request: \")\n",
        "  return req\n",
        "\n",
        "#function to calculate similarity using cosinus function\n",
        "def cosinus_calculate_similarity(weights_request, documents_weight):\n",
        "    #create empty doc for similarity\n",
        "    similarity_scores = {}\n",
        "    # iterate on request words and weight\n",
        "    for req_word, req_weight in weights_request.items(): #weigts_request.items is the weight of request\n",
        "        # if the request word exists in document weight dict\n",
        "        if req_word in documents_weight:\n",
        "            # iterate on documrents and weight\n",
        "            for doc, weight_doc in documents_weight[req_word].items():# documents_weight[req_word].items() is the weight of doc\n",
        "                #intialize key with a default similarity score of 0 to make sure each key has a value\n",
        "                similarity_scores.setdefault(doc, 0)\n",
        "                #give each doc a similarity swore using the cosinus equation\n",
        "                similarity_scores[doc] += weight_doc * req_weight / (sqrt(weight_doc) * sqrt(req_weight))\n",
        "    return similarity_scores\n",
        "\n",
        "\n",
        "# dice similarity function\n",
        "def dice_calculate_similarity(weights_request, documents_weight):\n",
        "   similarity_scores = {}\n",
        "   for req_word, req_weight in weights_request.items():\n",
        "        if req_word in documents_weight:\n",
        "            for doc, weight_doc in documents_weight[req_word].items():\n",
        "                similarity_scores.setdefault(doc, 0)\n",
        "                similarity_scores[doc] += 2 * (weight_doc * req_weight) / weight_doc + req_weight\n",
        "   return similarity_scores\n",
        "\n",
        "\n",
        "#jaccard similarity function\n",
        "def jaccard_calculate_similarity(weights_request, documents_weight):\n",
        "   similarity_scores = {}\n",
        "   for req_word, req_weight in weights_request.items():\n",
        "        if req_word in documents_weight:\n",
        "            for doc, weight_doc in documents_weight[req_word].items():\n",
        "                similarity_scores.setdefault(doc, 0)\n",
        "                similarity_scores[doc] += weight_doc * req_weight / (weight_doc + req_weight) - (weight_doc * req_weight)\n",
        "   return similarity_scores\n",
        "\n",
        "\n",
        "#function to  return a given number of relvent doc\n",
        "def relevant_documents(x,similarity_scores):\n",
        "    #sort documents based on the similarity score,decendant\n",
        "    sorted_documents = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_documents[:x]\n",
        "\n",
        "#create inversed document\n",
        "create_index_inv(source)\n",
        "\n",
        "#get the user's request\n",
        "req=saisie()\n",
        "\n",
        "#indexed request\n",
        "indx_req=liste_Stem(req,stop)\n",
        "print(\"\\n indexed request: \" ,indx_req)\n",
        "#calculate the document weight\n",
        "documents_weight = calcul_weight_doc(source)\n",
        "#calculate the request weight\n",
        "req_weight=calculate_request_weight(indx_req)\n",
        "print(\"\\n request weight: \",req_weight)\n",
        "\n",
        "# Calculate similarity between request and documents\n",
        "similarity_scores = cosinus_calculate_similarity(req_weight, documents_weight)\n",
        "print(\"\\n similarity_score: \",similarity_scores)\n",
        "\n",
        "#print the relevant documents\n",
        "x=int(input(\"\\n enter number of relevant ocs to show: \"))\n",
        "relv_doc=relevant_documents(x,similarity_scores)\n",
        "print(relv_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "inKu9npRz34t"
      }
    }
  ]
}